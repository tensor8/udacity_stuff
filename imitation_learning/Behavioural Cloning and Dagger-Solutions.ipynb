{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning: Behavioural Cloning and the DAGGER Algorithm\n",
    "\n",
    "# SOLUTIONS to 5.1\n",
    "\n",
    "\n",
    "In this lesson you'll use imitation learning to teach a student policy to mimic an expert demonstrator! This is an important technique in robotics research.\n",
    "\n",
    "We'll first try the behavioural cloning technique, which is a simple baseline for imitation learning. It can generate good policies, but they typically can't recover after making mistakes.\n",
    "\n",
    "We'll then try the DAGGER algorithm, which results in policies that can recover from their mistakes!\n",
    "\n",
    "You can then try the exercises at the end.\n",
    "\n",
    "\n",
    "1. Setup\n",
    "2. View the student and expert policies\n",
    "3. Run Behavioural Cloning\n",
    "4. Run the DAGGER algorithm\n",
    "5. Exercises!\n",
    "\n",
    "**This notebook doesn't need a GPU! You should be able to run it on a laptop CPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10b6ac870>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add parent dir to find package. Only needed for source code build, pip install doesn't need it.\n",
    "import os, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(os.path.dirname(currentdir))\n",
    "os.sys.path.insert(0,parentdir)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pybullet_envs\n",
    "import pybullet as p\n",
    "import os.path\n",
    "import time\n",
    "import torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Instantiate the Environment and Expert Demonstrator\n",
    "\n",
    "We have two versions of the environment:\n",
    "\n",
    "- `env_flagrun_with_rendering` which has a GUI for visualization\n",
    "- `env_flagrun_without_rendering` which runs very quickly but doesn't show any visualization.\n",
    "\n",
    "These will be useful later!\n",
    "\n",
    "**If you ever get an error from the physics server, you'll have to run this cell again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WalkerBase::__init__ start\n",
      "WalkerBase::__init__ start\n",
      "WalkerBase::__init__ start\n",
      "WalkerBase::__init__ start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesough/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "from utils import rollout_for_one_episode as rollout_for_one_episode\n",
    "from utils import rollout_for_n_episodes as rollout_for_n_episodes\n",
    "\n",
    "# shutdown any physics clients that already exist\n",
    "try: p.disconnect()\n",
    "except: pass\n",
    "\n",
    "# build the two versions of the environment\n",
    "env_flagrun_harder_with_rendering = gym.make(\"HumanoidFlagrunHarderBulletEnv-v0\")\n",
    "env_flagrun_harder_with_rendering.render(mode=\"human\")\n",
    "env_flagrun_harder_without_rendering = gym.make(\"HumanoidFlagrunHarderBulletEnv-v0\")\n",
    "\n",
    "\n",
    "env_flagrun_with_rendering = gym.make(\"HumanoidFlagrunBulletEnv-v0\")\n",
    "env_flagrun_with_rendering.render(mode=\"human\")\n",
    "env_flagrun_without_rendering = gym.make(\"HumanoidFlagrunBulletEnv-v0\")\n",
    "\n",
    "\n",
    "# instantiate the expert\n",
    "from flagrun_expert_demonstrator import *\n",
    "flagrun_expert = ExpertPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                                    env_flagrun_with_rendering.action_space)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Evaluate the Untrained Student Policy\n",
    "\n",
    "Our student policy is a two-layer neural net.\n",
    "\n",
    "Let's run the untrained student ten times, recording the reward so that we have a baseline for later.\n",
    "\n",
    "We'll use the second version of the environment, `env_flagrun_harder_without_rendering`, which runs very quickly but doesn't show any visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "score=58.07 in 202 frames\n",
      "episode 1\n",
      "score=31.85 in 205 frames\n",
      "episode 2\n",
      "score=110.01 in 217 frames\n",
      "episode 3\n",
      "score=61.25 in 208 frames\n",
      "episode 4\n",
      "score=79.96 in 199 frames\n",
      "episode 5\n",
      "score=101.68 in 197 frames\n",
      "episode 6\n",
      "score=90.66 in 210 frames\n",
      "episode 7\n",
      "score=60.28 in 210 frames\n",
      "episode 8\n",
      "score=79.20 in 201 frames\n",
      "episode 9\n",
      "score=102.77 in 217 frames\n",
      "Average Expert Score: 77.57290093279468\n"
     ]
    }
   ],
   "source": [
    "# instantiate an untrained student policy\n",
    "from model import StudentPolicy as StudentPolicy\n",
    "student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "\n",
    "\n",
    "rollout_data = rollout_for_n_episodes(n = 10,\n",
    "                                      policy = student_policy,\n",
    "                                      env = env_flagrun_harder_without_rendering,\n",
    "                                      render=False)\n",
    "\n",
    "mean_student_score = np.mean(rollout_data['scores'])\n",
    "\n",
    "print('Average Expert Score:', mean_student_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the untrained student policy for 1000 iterations and recorded the scores so you can get a less-noisy idea of the score:\n",
    "\n",
    "![](student_score_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Watch the Expert Demonstrator\n",
    "\n",
    "Now we'll visualize the expert demonstrator!\n",
    "\n",
    "You should be able to observe three distinct behaviours:\n",
    "\n",
    "- Running towards a target\n",
    "- Changing direction\n",
    "- Getting up after a fall\n",
    "\n",
    "You can click+drag on the expert to knock it over and see how it recovers.\n",
    "\n",
    "Later, we'll train the student policy to imitate the expert.\n",
    "\n",
    "CTRL+drag in the GUI to rotate the view. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0\n",
      "score=-391.49 in 1000 frames\n"
     ]
    }
   ],
   "source": [
    "flagrun_expert = ExpertPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                                    env_flagrun_harder_with_rendering.action_space)\n",
    "           \n",
    "rollout_data = rollout_for_n_episodes(1,\n",
    "                       flagrun_expert,\n",
    "                       env = env_flagrun_harder_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train the Student Policy with Behavioural Cloning\n",
    "\n",
    "In behavioural cloning, we run the expert policy and record all the *[state, action]* pairs. We then train a student policy (with supervised learning!) to directly imitate the expert's actions.\n",
    "\n",
    "We've provided a helper function, `train_model`, which will train the student policy with the recorded expert *[state, action]* pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling Out Expert\n",
      "episode 0\n",
      "score=540.15 in 1000 frames\n",
      "episode 1\n",
      "score=165.80 in 1000 frames\n",
      "episode 2\n",
      "score=-294.68 in 1000 frames\n",
      "episode 3\n",
      "score=21.66 in 1000 frames\n",
      "episode 4\n",
      "score=-26.69 in 1000 frames\n",
      "episode 5\n",
      "score=-61.50 in 1000 frames\n",
      "episode 6\n",
      "score=613.22 in 1000 frames\n",
      "episode 7\n",
      "score=-1189.48 in 1000 frames\n",
      "episode 8\n",
      "score=341.18 in 1000 frames\n",
      "episode 9\n",
      "score=-623.59 in 747 frames\n",
      "episode 10\n",
      "score=-228.70 in 224 frames\n",
      "episode 11\n",
      "score=99.59 in 803 frames\n",
      "episode 12\n",
      "score=324.99 in 1000 frames\n",
      "episode 13\n",
      "score=165.17 in 1000 frames\n",
      "episode 14\n",
      "score=-1184.41 in 1000 frames\n",
      "episode 15\n",
      "score=212.59 in 1000 frames\n",
      "episode 16\n",
      "score=157.22 in 1000 frames\n",
      "episode 17\n",
      "score=352.05 in 1000 frames\n",
      "episode 18\n",
      "score=-655.86 in 867 frames\n",
      "episode 19\n",
      "score=-128.46 in 1000 frames\n",
      "episode 20\n",
      "score=702.36 in 1000 frames\n",
      "episode 21\n",
      "score=559.42 in 1000 frames\n",
      "episode 22\n",
      "score=-1624.88 in 1000 frames\n",
      "episode 23\n",
      "score=-404.31 in 1000 frames\n",
      "episode 24\n",
      "score=-1038.13 in 1000 frames\n",
      "episode 25\n",
      "score=-1578.80 in 1000 frames\n",
      "episode 26\n",
      "score=457.18 in 1000 frames\n",
      "episode 27\n",
      "score=-176.95 in 1000 frames\n",
      "episode 28\n",
      "score=-125.87 in 1000 frames\n",
      "episode 29\n",
      "score=-543.17 in 1000 frames\n",
      "episode 30\n",
      "score=-118.39 in 1000 frames\n",
      "episode 31\n",
      "score=-332.62 in 765 frames\n",
      "episode 32\n",
      "score=702.86 in 1000 frames\n",
      "episode 33\n",
      "score=-430.10 in 1000 frames\n",
      "episode 34\n",
      "score=-984.53 in 1000 frames\n",
      "episode 35\n",
      "score=-697.44 in 1000 frames\n",
      "episode 36\n",
      "score=480.90 in 1000 frames\n",
      "episode 37\n",
      "score=611.62 in 1000 frames\n",
      "episode 38\n",
      "score=-119.60 in 1000 frames\n",
      "episode 39\n",
      "score=-186.69 in 1000 frames\n",
      "episode 40\n",
      "score=-38.06 in 1000 frames\n",
      "episode 41\n",
      "score=454.19 in 1000 frames\n",
      "episode 42\n",
      "score=-315.85 in 1000 frames\n",
      "episode 43\n",
      "score=-103.76 in 1000 frames\n",
      "episode 44\n",
      "score=-761.13 in 1000 frames\n",
      "episode 45\n",
      "score=321.05 in 1000 frames\n",
      "episode 46\n",
      "score=-619.03 in 931 frames\n",
      "episode 47\n",
      "score=-595.04 in 1000 frames\n",
      "episode 48\n",
      "score=503.70 in 1000 frames\n",
      "episode 49\n",
      "score=374.87 in 1000 frames\n",
      "Training Student Model\n",
      "Epoch: 0, Total loss: 0.14033687114715576\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9e3857b6bb1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Now run behavioural cloning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mbehavioural_cloning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflagrun_expert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudent_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9e3857b6bb1c>\u001b[0m in \u001b[0;36mbehavioural_cloning\u001b[0;34m(expert_policy, student_policy)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# train student policy with supervised learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training Student Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mstudent_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_rollout_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstudent_policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/misc/udacity/imitation_learning/utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(policy, training_data, num_epochs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mne\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}, Total loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import train_model as train_model\n",
    "from utils import Dataset as Dataset\n",
    "\n",
    "def behavioural_cloning(expert_policy, student_policy):\n",
    "    '''\n",
    "    Given an expert demonstrator and a student policy, perform\n",
    "    n iterations of dagger.\n",
    "    \n",
    "    '''\n",
    "    # collect initial expert demonstrations\n",
    "    n=10\n",
    "    print('Rolling Out Expert')\n",
    "    expert_rollout_data = rollout_for_n_episodes(50,\n",
    "                       expert_policy,\n",
    "                       env = env_flagrun_harder_without_rendering,\n",
    "                       render=False)\n",
    "    \n",
    "    # train student policy with supervised learning\n",
    "    print('Training Student Model')\n",
    "    student_policy = train_model(student_policy, expert_rollout_data, num_epochs = 500)\n",
    "    return student_policy\n",
    "\n",
    "\n",
    "student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "\n",
    "# Now run behavioural cloning\n",
    "behavioural_cloning(flagrun_expert, student_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(student_rollout_data['scores'], bins=50)\n",
    "plt.title('Histogram of behavioural-cloning student scores (in adversarial environment!) over 100 episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Watch the Trained Student Policy\n",
    "\n",
    "Note that behavioural cloning works suprisingly well! The student policy should be able to run and turn. \n",
    "\n",
    "However if the student falls over, it can't get back up, because the expert doesn't fall enough to produce much training data!\n",
    "\n",
    "The expert is too good to fail so the student never learns how to recover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_rollout_data = rollout_for_n_episodes(n=1000,\n",
    "                                            policy = student_policy,\n",
    "                                            env = env_flagrun_without_rendering,\n",
    "                                            render=False)\n",
    "\n",
    "mean_student_score = np.mean(student_rollout_data['scores'])\n",
    "std_student_score = np.std(student_rollout_data['scores'])\n",
    "print('Average Expert Score:', mean_student_score, 'Standard Deviation in Expert Score:', std_student_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_for_n_episodes(n = 30,\n",
    "                       policy = student_policy,\n",
    "                       env = env_flagrun_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My solution\n",
    "\n",
    "It works!\n",
    "\n",
    "![](flagrun_adv_fallover.gif)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
