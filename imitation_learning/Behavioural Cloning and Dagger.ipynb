{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning: Behavioural Cloning and the DAGGER Algorithm\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "In this exercise you'll use imitation learning to teach a student policy to mimic an expert demonstrator. This is an important technique in robotics research.\n",
    "\n",
    "We'll first try the behavioural cloning technique, which is a simple baseline for imitation learning. It can generate good policies, but they typically can't recover after making mistakes.\n",
    "\n",
    "We'll then try the DAGGER algorithm, which results in policies that can recover from their mistakes!\n",
    "\n",
    "You can then try the exercises at the end.\n",
    "\n",
    "\n",
    "1. Setup\n",
    "2. View the student and expert policies\n",
    "3. Run Behavioural Cloning\n",
    "4. Run the DAGGER algorithm\n",
    "5. Exercises!\n",
    "\n",
    "<br>\n",
    "<img src=\"flagrun_adv_fallover.gif\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<br>\n",
    "\n",
    "**This notebook doesn't need a GPU! You should be able to run it on a laptop CPU.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add parent dir to find package. Only needed for source code build, pip install doesn't need it.\n",
    "import os, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(os.path.dirname(currentdir))\n",
    "os.sys.path.insert(0,parentdir)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pybullet_envs\n",
    "import pybullet as p\n",
    "import os.path\n",
    "import time\n",
    "import torch\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Instantiate the Environment and Expert Demonstrator\n",
    "\n",
    "We have two versions of the environment:\n",
    "\n",
    "- `env_flagrun_with_rendering` which has a GUI for visualization\n",
    "- `env_flagrun_without_rendering` which runs very quickly but doesn't show any visualization.\n",
    "\n",
    "These will be useful later!\n",
    "\n",
    "**If you ever get an error from the physics server, you'll have to run this cell again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import rollout_for_one_episode as rollout_for_one_episode\n",
    "from utils import rollout_for_n_episodes as rollout_for_n_episodes\n",
    "\n",
    "# shutdown any physics clients that already exist\n",
    "try: p.disconnect()\n",
    "except: pass\n",
    "\n",
    "# build the two versions of the environment\n",
    "env_flagrun_with_rendering = gym.make(\"HumanoidFlagrunBulletEnv-v0\")\n",
    "env_flagrun_with_rendering.render(mode=\"human\")\n",
    "env_flagrun_without_rendering = gym.make(\"HumanoidFlagrunBulletEnv-v0\")\n",
    "\n",
    "\n",
    "# instantiate the expert\n",
    "from flagrun_expert_demonstrator import *\n",
    "flagrun_expert = ExpertPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                                    env_flagrun_with_rendering.action_space)\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Evaluate the Untrained Student Policy\n",
    "\n",
    "Our student policy is a two-layer neural net.\n",
    "\n",
    "Let's run the untrained student ten times, recording the reward so that we have a baseline for later.\n",
    "\n",
    "We'll use the second version of the environment, `env_flagrun_without_rendering`, which runs very quickly but doesn't show any visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate an untrained student policy\n",
    "from model import StudentPolicy as StudentPolicy\n",
    "student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "\n",
    "\n",
    "rollout_data = rollout_for_n_episodes(n = 30,\n",
    "                                      policy = student_policy,\n",
    "                                      env = env_flagrun_without_rendering,\n",
    "                                      render=False)\n",
    "\n",
    "mean_student_score = np.mean(rollout_data['scores'])\n",
    "\n",
    "print('Average Untrained Student Score:', mean_student_score)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(rollout_data['scores'])\n",
    "plt.title('Histogram of Untrained Student Scores over 100 Episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the untrained student policy for 1000 iterations so you can get a less-noisy idea of the distribution of scores:\n",
    "\n",
    "![](student_score_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Watch the Untrained Student Policy\n",
    "\n",
    "\n",
    "Now we'll use the first version of the environment, `env_flagrun_with_rendering`, which runs in real-time and creates a GUI for visualization.\n",
    "\n",
    "Watch few rollouts in the GUI: the humanoid will fall to the floor because the policy hasn't been trained yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the untrained student policy (it should flop on the floor!)\n",
    "rollout_data = rollout_for_n_episodes(n = 3,\n",
    "                       policy = student_policy,\n",
    "                       env = env_flagrun_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluate the Expert Demonstrator\n",
    "\n",
    "We have a pretrained expert policy from the pybullet gym. Let's run this expert ten times, recording the reward so that we have a baseline for later.\n",
    "\n",
    "We'll use the second version of the environment, `env_flagrun_without_rendering`, which runs very quickly but doesn't show any visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_data = rollout_for_n_episodes(n = 10,\n",
    "                                      policy = ExpertPolicy(env_flagrun_without_rendering.observation_space,\n",
    "                                      env_flagrun_without_rendering.action_space),\n",
    "                                      env = env_flagrun_without_rendering,\n",
    "                                      render=False)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(rollout_data['scores'])\n",
    "plt.title('Histogram of Expert Scores over 100 episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the untrained student policy for 1000 iterations so you get a less-noisy idea of the score:\n",
    "\n",
    "![](expert_score_histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Watch the Expert Demonstrator\n",
    "\n",
    "Plotting the agent's score can be useful for a quantative comparison, but it's always important to visualize a rollout so you know what's actually happening.\n",
    "\n",
    "Run the next cell to visualize the expert demonstrator!\n",
    "\n",
    "You can click+drag on the expert to knock it over and see how it recovers.\n",
    "\n",
    "You should be able to observe three distinct behaviours:\n",
    "\n",
    "- Running towards a target\n",
    "- Changing direction\n",
    "- Getting up after a fall\n",
    "\n",
    "Later, we'll train the student policy to imitate the expert.\n",
    "\n",
    "CTRL+drag in the GUI to rotate the view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flagrun_expert = ExpertPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                                    env_flagrun_with_rendering.action_space)\n",
    "           \n",
    "rollout_data = rollout_for_n_episodes(1,\n",
    "                       flagrun_expert,\n",
    "                       env = env_flagrun_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll aim to hit an average score of about 500 with our student policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train the Student Policy with Behavioural Cloning\n",
    "\n",
    "In behavioural cloning, we run the expert policy and record all the *[state, action]* pairs. We then train a student policy (with supervised learning!) to directly imitate the expert's actions.\n",
    "\n",
    "We've provided a helper function, `train_model`, which will train the student policy with the recorded expert *[state, action]* pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_model as train_model\n",
    "from utils import Dataset as Dataset\n",
    "\n",
    "def behavioural_cloning(expert_policy, student_policy):\n",
    "    '''\n",
    "    Given an expert demonstrator and a student policy, perform\n",
    "    n iterations of dagger.\n",
    "    \n",
    "    '''\n",
    "    # collect initial expert demonstrations\n",
    "    n=10\n",
    "    print('Rolling Out Expert')\n",
    "    expert_rollout_data = rollout_for_n_episodes(30,\n",
    "                       expert_policy,\n",
    "                       env = env_flagrun_without_rendering,\n",
    "                       render=False)\n",
    "    \n",
    "    # train student policy with supervised learning\n",
    "    print('Training Student Model')\n",
    "    student_policy = train_model(student_policy, expert_rollout_data, num_epochs = 20)\n",
    "    return student_policy\n",
    "\n",
    "\n",
    "student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "\n",
    "# Now run behavioural cloning\n",
    "behavioural_cloning(flagrun_expert, student_policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate the Trained Student Policy\n",
    "\n",
    "Let's record the average score so we can quantatively compare the student to the expert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_rollout_data = rollout_for_n_episodes(n=10,\n",
    "                                            policy = student_policy,\n",
    "                                            env = env_flagrun_without_rendering,\n",
    "                                            render=False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(rollout_data['scores'])\n",
    "plt.title('Histogram of Behavioural Cloning-Trained Student Scores over 100 episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the behavioural cloning-trained student policy for 1000 iterations so you get a less noisy idea of the score:\n",
    "\n",
    "![](student_bc_score_histogram.png)\n",
    "\n",
    "Compare this to the expert!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Watch the Trained Student Policy\n",
    "\n",
    "Visualize the student -- how does it qualiatively compare to the expert?\n",
    "\n",
    "Make sure to  can click+drag on the humanoid in the GUI to knock it over and see how it recovers.\n",
    "\n",
    "Note that behavioural cloning works suprisingly well! The student policy should be able to run and turn. \n",
    "\n",
    "However if the student falls over, it has trouble getting back up, because the expert doesn't fall enough to produce much training data!\n",
    "\n",
    "The expert is too good to fail so the student never learns how to recover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_for_n_episodes(n = 10,\n",
    "                       policy = student_policy,\n",
    "                       env = env_flagrun_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Train the Agent with the DAGGER Algorithm\n",
    "\n",
    "How can we train the student to get back up when it falls?\n",
    "\n",
    "One way is to ask the expert *what it would have done if it had fallen*, and then train on that data.\n",
    "\n",
    "This is the essence of the DAGGER algorithm!\n",
    "\n",
    "In the DAGGER algorithm, we first train a student policy with behavioural cloning. We then rollout this trained student policy and record all the states it visits (it'll fall over a lot!). We then run these states through the expert policy (asking the expert what it *would have done*), generating expert actions, and use these new [state, expert_action] pairs as extra training data for another iteration of behavioural cloning. We can repeat this algorithm many times.\n",
    "\n",
    "We've provided a helper function, train_model, which will train the student policy with the recorded expert [state, action] pairs.\n",
    "\n",
    "The first few iterations should give a similar result to behavioural cloning.\n",
    "\n",
    "By the final iteration, the agent should be able to stand up when it falls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import torch\n",
    "from torch.autograd import Variable\n",
    "from utils import train_model as train_model\n",
    "\n",
    "def evaluate_expert(policy, data):\n",
    "    '''\n",
    "    Evaluate an expert policy on a list of recorded observations.\n",
    "    What would it have done?\n",
    "    '''\n",
    "    actions = []\n",
    "    for obs in data:\n",
    "        obs = Variable(torch.Tensor(obs))\n",
    "        predicted_action = policy(obs)\n",
    "        actions.append(predicted_action.data.numpy())\n",
    "    return actions\n",
    "\n",
    "def dagger(expert_policy, student_policy, n_dagger_iterations, env=env_flagrun_without_rendering):\n",
    "    '''\n",
    "    Given an expert demonstrator and a student policy, perform\n",
    "    n iterations of DAGGER.\n",
    "    \n",
    "    '''\n",
    "    # collect initial expert demonstrations\n",
    "    n=10\n",
    "    print('Rolling Out Expert')\n",
    "    expert_rollout_data = rollout_for_n_episodes(n=10,\n",
    "                                            policy = expert_policy,\n",
    "                                            env = env_flagrun_without_rendering,\n",
    "                                            render=False)    \n",
    "    \n",
    "    print('Training Student')\n",
    "    # train initial student model with behavioural cloning\n",
    "    training_data = expert_rollout_data\n",
    "    trained_student = train_model(student_policy, expert_rollout_data, num_epochs = 10)\n",
    "    \n",
    " \n",
    "    for i in range(n_dagger_iterations):\n",
    "        print('Iteration', i, 'of DAGGER')\n",
    "        \n",
    "        # rollout student model\n",
    "        student_rollout_data = rollout_for_n_episodes(n=10,\n",
    "                                            policy = student_policy,\n",
    "                                            env = env_flagrun_without_rendering,\n",
    "                                            render=False)\n",
    "        \n",
    "        # evaluate expert actions on student's trajectories and add to dataset\n",
    "        expert_corrections = evaluate_expert(expert_policy, student_rollout_data['observations'])\n",
    "        \n",
    "\n",
    "        training_data = {'observations': training_data['observations'] + student_rollout_data['observations'],\n",
    "                         'actions':      training_data['actions']      + expert_corrections}\n",
    "        \n",
    "        # train student model with behavioural cloning \n",
    "        assert len(training_data['observations']) == len(training_data['actions'])\n",
    "\n",
    "        student_policy =  train_model(student_policy, training_data, num_epochs = 30)\n",
    "        \n",
    "    return student_policy\n",
    "\n",
    "\n",
    "# instantiate a new untrained student policy\n",
    "torch.manual_seed(0)\n",
    "student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "\n",
    "dagger(flagrun_expert,\n",
    "       student_policy,\n",
    "       n_dagger_iterations = 10,\n",
    "       env = env_flagrun_without_rendering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: we've provided you with a student model pre-trained with DAGGER. Run the following cell to load it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_policy = StudentPolicy(env_flagrun_with_rendering.observation_space,\n",
    "                               env_flagrun_with_rendering.action_space)  \n",
    "student_policy.load_state_dict(torch.load('trained_dagger_agent.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluate the DAGGER-Trained Student Policy\n",
    "\n",
    "Let's record the average score so we can quantatively compare the student to the expert, and the behavioural-cloning student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_data = rollout_for_n_episodes(n = 100,\n",
    "                                       policy = student_policy,\n",
    "                                       env = env_flagrun_without_rendering,\n",
    "                                       render=False)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(rollout_data['scores'], bins = 30)\n",
    "plt.title('Histogram of DAGGER-trained student scores over 100 episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran the DAGGER-trained student policy for 1000 iterations so you get a less noisy idea of the score:\n",
    "\n",
    "![](student_dagger_score_histogram.png)\n",
    "\n",
    "Compare this to the expert! The shape of the curve doesn't particularly matter (you'll get a different curve each time you run)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Watch the DAGGER-trained student policy\n",
    "\n",
    "You should be able to see the agent stand up after falling! Remember you can click+drag on the humanoid in the GUI to knock it over and see how it recovers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_for_n_episodes(n = 5,\n",
    "                       policy = student_policy,\n",
    "                       env = env_flagrun_with_rendering,\n",
    "                       render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore\n",
    "\n",
    "In this exercise, we have implemented the behavioural cloning and DAGGER algorithms, and demonstrated how to train policies using imitation learning that can *recover from their mistakes!*.\n",
    "\n",
    "To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "\n",
    "### 5.1 Adversarial Environments\n",
    "\n",
    "Another way to teach student policies to recover is to make an environment so difficult that it *forces the expert to fail!*\n",
    "\n",
    "You can then see what they do to recover, and use this as training data.\n",
    "\n",
    "Try this out! The `HumanoidFlagrunHarderBulletEnv-v0` environment is the same as before, except fast blocks are thrown directly at the humanoid to knock it over. Gather expert data in this environment, and then test in the original `HumanoidFlagrunBulletEnv-v0` environment.\n",
    "\n",
    "Now you should be able to train a flagrun agent that can recover from a fall with basic behavioural cloning, without needing DAGGER! Try it!\n",
    "\n",
    "### 5.2 Imitation as an objective during RL training\n",
    "\n",
    "We've considered two basic approaches to imitation learning that don't require intensive RL algotithms like you have used in previous sections, instead relying upon having direct access to the observation space and action space of an expert demonstrator (e.g. recorded commands from a car's data bus as a human demonstrator drives around!).\n",
    "\n",
    "These approaches won't work in other situations. For instance, imagine you're trying to train a [humanoid robot](https://www.youtube.com/watch?v=LikxFZZO2sk) with imitation learning. Where is the expert data? We can watch a person running, but we can't record their joint torques! (And even if we could, they wouldn't transfer to a robot.)\n",
    "\n",
    "More powerful imitation learning approaches can be used in this situation to *guide* the reinforcement learning process.\n",
    "\n",
    "One examples is [DEEPMIMIC](https://bair.berkeley.edu/blog/2018/04/10/virtual-stuntman/), which combines a motion-imitation objective with the task objective, and ends up producing motions that are far more natural-looking than with [traditional RL approaches](https://www.youtube.com/watch?v=hx_bgoTF7bs).\n",
    "\n",
    "Read about this approach. Can you get the [code](https://github.com/xbpeng/DeepMimic) working?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
